# Cheddar Bench Report

Cheddar Bench evaluates CLI coding agents on blind bug detection using self-play:
agents inject bugs into repos, other agents review those repos without seeing ground truth.

## Headline Results

Scale:
- 50 repositories
- 150 challenges (3 challengers x 50 repos)
- 450 reviews (3 reviewers x 150 challenges)
- 2,603 injected bugs total

Scoring policy for this report:
- matcher consumes raw reviewer `bugs/*.json` payloads
- matcher repeated 5 times per review
- median run selected (`repeat=5`, `aggregate=median`)

### Weighted (global bug count)

Weighted bugs found (%):

```text
ðŸŸ© Claude: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (58.05%)
ðŸŸ§ Codex:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (37.84%)
ðŸŸ¦ Gemini: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (27.81%)
```

| Reviewer | Bugs Found |
|----------|------------|
| Claude | **1,511 / 2,603 (58.05%)** |
| Codex | **985 / 2,603 (37.84%)** |
| Gemini | **724 / 2,603 (27.81%)** |

### Unweighted (mean of 150 challenge scores)

| Reviewer | Detection Rate |
|----------|----------------|
| Claude | **61.65%** |
| Codex | **43.17%** |
| Gemini | **34.64%** |

Why two metrics:

- Weighted emphasizes bug-level recall across the full benchmark (`sum(bugs_found) / sum(total_bugs)`).
- Unweighted emphasizes consistency across repositories (each challenge contributes equally).

## Key Findings

- Claude leads by a wide margin on both unweighted and weighted metrics.
- Codex is second and notably behind Claude on broad recall.
- Gemini trails both on overall bug detection.
- Extreme per-challenge variance exists, so single-challenge anecdotes are noisy.

## Self Detection (Weighted)

Self = reviewer analyzes challenges generated by the same agent family (for example, Codex on
`codex-*`). Delta is measured against each reviewer's overall weighted score in this report.

| Reviewer | Self (Weighted) | Delta vs Total |
|----------|------------------|----------------|
| Claude | 52.68% | -5.37 pp |
| Codex | 39.72% | +1.88 pp |
| Gemini | 30.88% | +3.07 pp |

## Methodology

1. Challenge agent injects bugs into a clean repo; the target bug count scales with repository size and is capped (currently 24 per challenge).
2. Ground truth is the challenger-produced `bugs.json` manifest (which includes the injected bug set and metadata).
3. Reviewer agent audits the mutated repo blind and emits findings as `bugs/*.json` payloads (plus optional free-form text output).
4. LLM matcher assigns reviewer raw finding payloads to injected bugs and computes score.

Notes:
- This benchmark compares CLI tools as full systems (prompting, tool/runtime behavior, and CLI flag configuration), not just base models.
- Runs execute with autonomous permissions (`--yolo` / dangerous-skip equivalents) on sandboxed challenge repositories.
- Scoring consumes raw reviewer `bugs/*.json` payloads as emitted by agents.
- Repeat+median is used to reduce matcher stochasticity.

## Dataset

The full `challenges/` snapshot is published here:

- [challenges archive (.tar.gz)](https://cheddar-bench-data-public.s3.eu-central-1.amazonaws.com/datasets/cheddar-bench-challenges-2026-02-21T122452Z-cb4b7ba38c3c-r2/cheddar-bench-challenges-2026-02-21T122452Z-cb4b7ba38c3c-r2.tar.gz)

For checksums, object version IDs, and retrieval commands, see the Dataset section in `README.md`.
